{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca683ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch accelerate peft sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12e7a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ucloud/.local/lib/python3.12/site-packages (4.36.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b9be8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq,AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import random\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "444c9a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating en split: 100%|██████████| 5000/5000 [00:00<00:00, 832170.15 examples/s]\n",
      "Generating ru split: 100%|██████████| 5000/5000 [00:00<00:00, 1676246.50 examples/s]\n",
      "Generating uk split: 100%|██████████| 5000/5000 [00:00<00:00, 1829975.57 examples/s]\n",
      "Generating de split: 100%|██████████| 5000/5000 [00:00<00:00, 1177844.43 examples/s]\n",
      "Generating es split: 100%|██████████| 5000/5000 [00:00<00:00, 1376806.72 examples/s]\n",
      "Generating am split: 100%|██████████| 5000/5000 [00:00<00:00, 1364267.50 examples/s]\n",
      "Generating zh split: 100%|██████████| 5000/5000 [00:00<00:00, 2239111.68 examples/s]\n",
      "Generating ar split: 100%|██████████| 5000/5000 [00:00<00:00, 1500108.73 examples/s]\n",
      "Generating hi split: 100%|██████████| 5000/5000 [00:00<00:00, 1008585.58 examples/s]\n",
      "Generating it split: 100%|██████████| 5000/5000 [00:00<00:00, 1569607.07 examples/s]\n",
      "Generating fr split: 100%|██████████| 5000/5000 [00:00<00:00, 1768554.56 examples/s]\n",
      "Generating he split: 100%|██████████| 2011/2011 [00:00<00:00, 1025251.65 examples/s]\n",
      "Generating hin split: 100%|██████████| 4363/4363 [00:00<00:00, 1273823.50 examples/s]\n",
      "Generating tt split: 100%|██████████| 5000/5000 [00:00<00:00, 1672103.33 examples/s]\n",
      "Generating ja split: 100%|██████████| 5000/5000 [00:00<00:00, 1569842.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_tox_ds = load_dataset(\"textdetox/multilingual_toxicity_dataset\",split=\"en\")\n",
    "ru_tox_ds = load_dataset(\"textdetox/multilingual_toxicity_dataset\",split=\"ru\")\n",
    "uk_tox_ds = load_dataset(\"textdetox/multilingual_toxicity_dataset\",split=\"uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b47842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_non_toxic = [en for en in en_tox_ds if en['toxic']==0]\n",
    "ru_non_toxic = [ru for ru in ru_tox_ds if ru['toxic']==0]\n",
    "uk_non_toxic = [uk for uk in uk_tox_ds if uk['toxic']==0]\n",
    "\n",
    "random.seed(42)\n",
    "en_id = random.sample(en_non_toxic, 2000)\n",
    "ru_id = random.sample(ru_non_toxic, 1000)\n",
    "uk_id = random.sample(uk_non_toxic, 470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a74b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_identity_pairs(samples, lang_code):\n",
    "    return [{\"toxic\": s['text'], \"clean\": s['text'], \"lang\": lang_code} for s in samples]\n",
    "\n",
    "all_identity_pairs = (\n",
    "    build_identity_pairs(en_id, \"en\") +\n",
    "    build_identity_pairs(ru_id, \"ru\") +\n",
    "    build_identity_pairs(uk_id, \"uk\")\n",
    ")\n",
    "identity_dataset = Dataset.from_list(all_identity_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b2fdd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 19744/19744 [00:00<00:00, 670930.97 examples/s]\n",
      "Generating train split: 100%|██████████| 11090/11090 [00:00<00:00, 281870.49 examples/s]\n",
      "Generating validation split: 100%|██████████| 1116/1116 [00:00<00:00, 193559.25 examples/s]\n",
      "Generating train split: 100%|██████████| 3893/3893 [00:00<00:00, 273700.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_ds = load_dataset(\"s-nlp/paradetox\", split=\"train\")\n",
    "ru_ds = load_dataset(\"s-nlp/ru_paradetox\", split=\"train\")\n",
    "uk_ds = load_dataset(\"textdetox/uk_paradetox\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a86061",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_MAP = {\n",
    "    \"en\": (\"en_toxic_comment\", \"en_neutral_comment\"),\n",
    "    \"ru\": (\"ru_toxic_comment\", \"ru_neutral_comment\"),\n",
    "    \"uk\": (\"toxic_sentence\",  \"neutral_sentence\"),\n",
    "    \"es\": (\"toxic_sentence\",  \"neutral_sentence\"),\n",
    "    \"zh\": (\"toxic_sentence\",  \"neutral_sentence\")\n",
    "}\n",
    "\n",
    "def make_strip(lang):\n",
    "    s, t = COL_MAP[lang]\n",
    "    return lambda ex: {\n",
    "        \"toxic\": ex[s],\n",
    "        \"clean\": ex[t],\n",
    "        \"lang\": lang  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7fd1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 19744/19744 [00:00<00:00, 47841.74 examples/s]\n",
      "Map: 100%|██████████| 11090/11090 [00:00<00:00, 46197.27 examples/s]\n",
      "Map: 100%|██████████| 3893/3893 [00:00<00:00, 46604.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "en_ds = en_ds.map(make_strip(\"en\"), remove_columns=en_ds.column_names)\n",
    "ru_ds = ru_ds.map(make_strip(\"ru\"), remove_columns=ru_ds.column_names)\n",
    "uk_ds = uk_ds.map(make_strip(\"uk\"), remove_columns=uk_ds.column_names)\n",
    "toxic_dataset = concatenate_datasets([en_ds, ru_ds, uk_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbbbb833",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = concatenate_datasets([identity_dataset,toxic_dataset]).shuffle(seed=42)\n",
    "split = full_ds.train_test_split(test_size=0.05)\n",
    "train_ds = split['train']\n",
    "dev_ds = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b9f60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"bigscience/mt0-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(base_model_name, device_map=\"auto\",torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38f569f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,874,368 || all params: 1,248,455,680 || trainable%: 1.5118\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=32, #64\n",
    "    lora_alpha=64, #128\n",
    "    target_modules=[\"q\",\"v\",\"k\",\"o\"],\n",
    "    lora_dropout=0.05 #0.05\n",
    ")\n",
    "model = get_peft_model(base, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e60fc9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG_PROMPT_MAP = {\n",
    "    \"en\": \"Rewrite the sentence by replacing toxic or offensive words with neutral and polite expressions. Preserve the original meaning\",\n",
    "    \"ru\": \"Перепишите предложение, заменив токсичные или грубые слова на нейтральные и вежливые. Смысл предложения должен сохраняться.\",\n",
    "    \"uk\": \"Перепишіть речення, замінивши токсичні або образливі слова на нейтральні й ввічливі. Зміст має залишатися незмінним.\",\n",
    "    \"zh\": \"请将句子中的粗俗或攻击性词语改写为中性、礼貌的表达，保留原句意思，避免不必要的删改。\"\n",
    "}\n",
    "\n",
    "def add_lang_prefix(example):\n",
    "    prefix = LANG_PROMPT_MAP.get(example[\"lang\"], LANG_PROMPT_MAP[\"en\"])\n",
    "    model_inputs = tokenizer(\n",
    "        prefix + example[\"toxic\"],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=256\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        text_target=example[\"clean\"],\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=256\n",
    "    )\n",
    "    input_ids = targets[\"input_ids\"]\n",
    "    labels = input_ids if isinstance(input_ids[0], int) else input_ids[0]\n",
    "    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3df2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = train_ds.map(add_lang_prefix, remove_columns=train_ds.column_names)\n",
    "train_tok.set_format(\"torch\")\n",
    "\n",
    "dev_tok = dev_ds.map(add_lang_prefix, remove_columns=dev_ds.column_names)\n",
    "dev_tok.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ff880a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19538/1169277510.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MyTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = MyTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13608' max='13608' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13608/13608 1:01:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.142400</td>\n",
       "      <td>0.946961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.055800</td>\n",
       "      <td>0.863434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.987000</td>\n",
       "      <td>0.847739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.808196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.964200</td>\n",
       "      <td>0.797166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.929200</td>\n",
       "      <td>0.783389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.886700</td>\n",
       "      <td>0.779446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.858700</td>\n",
       "      <td>0.777512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.770090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.768276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.831800</td>\n",
       "      <td>0.773126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.808100</td>\n",
       "      <td>0.766266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.748546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.856100</td>\n",
       "      <td>0.743003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.738595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.873300</td>\n",
       "      <td>0.746590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.848500</td>\n",
       "      <td>0.740887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.835800</td>\n",
       "      <td>0.741494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.838700</td>\n",
       "      <td>0.742648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.813600</td>\n",
       "      <td>0.737581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.735506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.819500</td>\n",
       "      <td>0.734617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.820800</td>\n",
       "      <td>0.736686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>0.734644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.867900</td>\n",
       "      <td>0.735971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.736234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>0.735817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('mt0l-lora-adapter-largelearning_low/tokenizer_config.json',\n",
       " 'mt0l-lora-adapter-largelearning_low/special_tokens_map.json',\n",
       " 'mt0l-lora-adapter-largelearning_low/spiece.model',\n",
       " 'mt0l-lora-adapter-largelearning_low/added_tokens.json',\n",
       " 'mt0l-lora-adapter-largelearning_low/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer,EarlyStoppingCallback \n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "# ✅ 训练参数\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"ckpt_mt0_large_largelearning_Low\",\n",
    "    #batch size\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    #training epoch\n",
    "    num_train_epochs=3,\n",
    "    #优化器\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    label_smoothing_factor=0.1, \n",
    "    # ---------- 日志 / 保存 ----------\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    #——————精度\n",
    "    fp16=False,\n",
    "    bf16=True,  \n",
    "    half_precision_backend=\"auto\", \n",
    "    #\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "class MyTrainer(Seq2SeqTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss  \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = MyTrainer( \n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"mt0l-lora-adapter-largelearning_low\")\n",
    "tokenizer.save_pretrained(\"mt0l-lora-adapter-largelearning_low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7d1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
