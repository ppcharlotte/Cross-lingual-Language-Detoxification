{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3bd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers torch accelerate peft sentencepiece sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1eb636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ucloud/.local/lib/python3.12/site-packages (4.36.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ucloud/.local/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucloud/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucloud/.local/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.36.2\n",
      "    Uninstalling transformers-4.36.2:\n",
      "      Successfully uninstalled transformers-4.36.2\n",
      "Successfully installed tokenizers-0.21.1 transformers-4.52.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df6739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ucloud/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq,AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from peft import LoraConfig, get_peft_model,PeftModel, PeftConfig\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a0314",
   "metadata": {},
   "source": [
    "## SPANISH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0648b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "sim = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "classify_tok = AutoTokenizer.from_pretrained(\"textdetox/xlmr-large-toxicity-classifier\")\n",
    "classify_model = AutoModelForSequenceClassification.from_pretrained(\"textdetox/xlmr-large-toxicity-classifier\")\n",
    "classify_model.eval()\n",
    "\n",
    "\n",
    "def score_model(text):\n",
    "    with torch.no_grad():\n",
    "        inputs = classify_tok(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(classify_model.device) for k, v in inputs.items()}\n",
    "        logits = classify_model(**inputs).logits\n",
    "        probs = F.softmax(logits, dim=-1)  # 分类器输出两个类：toxic 和 non-toxic\n",
    "        toxic_score = probs[0][1].item()   # 索引1对应 toxic 类\n",
    "        return toxic_score\n",
    "\n",
    "\n",
    "toxic_words = set()\n",
    "#zh_lexicon = load_dataset(\"textdetox/multilingual_toxic_lexicon\", split=\"zh\")\n",
    "#zh_toxic_words = set(zh_lexicon[\"text\"])\n",
    "es_lexicon = load_dataset(\"textdetox/multilingual_toxic_lexicon\", split=\"es\")\n",
    "es_toxic_words = set(es_lexicon[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc33f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_infer_model(adapter_path=\"mt0l-lora-adapter-largelearning_low\"):\n",
    "    peft_cfg = PeftConfig.from_pretrained(adapter_path)\n",
    "    base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        peft_cfg.base_model_name_or_path,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path).eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
    "    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "    \n",
    "def detoxify(text: str,\n",
    "             model,\n",
    "             tokenizer,\n",
    "             score_model,  # STA分类器\n",
    "             sim_model,  # sim模型（如 sentence-transformers）\n",
    "             max_new_tokens: int = 64,\n",
    "             num_return_sequences: int = 10) -> dict:\n",
    "\n",
    "    #prefix = \"请将句子中的粗俗或攻击性词语改写为中性、礼貌的表达，保留原句意思，避免不必要的删改。\"\n",
    "    #prefix = \"Formuliere den Satz um, indem du toxische oder beleidigende Wörter durch neutrale und höfliche Ausdrücke ersetzt. Erhalte dabei die ursprüngliche Bedeutung.\"\n",
    "    prefix = \"Reescribe la frase reemplazando términos ofensivos por lenguaje neutral, sin cambiar su significado.\"\n",
    "    inputs = tokenizer(prefix + text,\n",
    "                       return_tensors=\"pt\",\n",
    "                       truncation=True,\n",
    "                       max_length=192).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        no_repeat_ngram_size=4,\n",
    "        repetition_penalty=3.0, \n",
    "        num_beams=10, \n",
    "        num_return_sequences=num_return_sequences,      \n",
    "        early_stopping=True,\n",
    "        do_sample=False,\n",
    "        decoder_start_token_id=tokenizer.pad_token_id  \n",
    "    )\n",
    "\n",
    "    candidates = tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    seen = set()\n",
    "    unique_candidates = []\n",
    "    for c in candidates:\n",
    "        c = c.strip()\n",
    "        if c and c not in seen:\n",
    "            seen.add(c)\n",
    "            unique_candidates.append(c)\n",
    "\n",
    "    # filter toxic lexicon\n",
    "    def contains_toxic_word(text):\n",
    "        return any(word in text for word in toxic_words)\n",
    "\n",
    "    \n",
    "    filtered = [c for c in unique_candidates if not contains_toxic_word(c)]\n",
    "    final_candidates = filtered if filtered else unique_candidates\n",
    "\n",
    "    # select best candidate\n",
    "    def select_best_output(toxic_text, detoxs, sta_model, sim_model):\n",
    "        vals = []\n",
    "        for detox in detoxs:\n",
    "            emb = sim_model.encode([toxic_text, detox], convert_to_tensor=True)\n",
    "            sim_val = (emb[0] * emb[1]).sum()\n",
    "            sta_score = 1 - score_model(detox)\n",
    "            vals.append((detox, (sim_val * sta_score).item()))\n",
    "        best, _ = max(vals, key=lambda x: x[1])\n",
    "        return best\n",
    "\n",
    "    best_text = select_best_output(text, final_candidates, score_model, sim_model)\n",
    "\n",
    "    return {\n",
    "        \"best\": best_text,\n",
    "        \"candidates\": final_candidates\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf42a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "es_ds = load_dataset(\"textdetox/multilingual_paradetox\", split=\"es\")\n",
    "#zh_ds = load_dataset(\"textdetox/multilingual_paradetox\", split=\"zh\")\n",
    "\n",
    "\n",
    "toxic_list = []\n",
    "neutral_list = []\n",
    "lang_list = []\n",
    "\n",
    "model, tok = load_infer_model()\n",
    "\n",
    "\n",
    "for txt in es_ds['toxic_sentence']:\n",
    "    detoxed = detoxify(txt, model, tok, score_model, sim)\n",
    "\n",
    "    toxic_list.append(txt)\n",
    "    neutral_list.append(detoxed[\"best\"])\n",
    "    lang_list.append(\"es\")\n",
    "    #lang_list.append(\"zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "976a186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"toxic_sentence\": toxic_list,\n",
    "    \"neutral_sentence\": neutral_list,\n",
    "    \"lang\": lang_list\n",
    "})\n",
    "\n",
    "# 检查并替换 NaN（官方要求）\n",
    "df.fillna(value={\"neutral_sentence\": df[\"toxic_sentence\"]}, inplace=True)\n",
    "\n",
    "# 保存为 .tsv 文件\n",
    "df.to_csv(\"submission_zhlow_baslin.tsv\", sep=\"\\t\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
